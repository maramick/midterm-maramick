{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgcLmLqTIyP9"
      },
      "source": [
        "# Basic Understaning about NEURAL STYLE TRANSFER algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUzyVF1pfw4L"
      },
      "source": [
        "### Set Environment & Downloading Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ld0h0Hys0TD5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "disable_eager_execution()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmOZvdEOc2HU",
        "outputId": "c3821a73-ebda-431e-8a82-8d61f89ed749"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import pandas as pd\n",
        "print('tensorflow version = ' + tf.__version__)\n",
        "print('numpy version = ' + np.__version__)\n",
        "print('pandas version = ' + pd.__version__)\n",
        "print('keras version = ' + keras.__version__)\n",
        "import sys\n",
        "print('python version = ' + sys.version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaDHh6jjW40n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import numpy as np\n",
        "import os\n",
        "from keras import backend as K\n",
        "from keras_preprocessing.image import load_img, save_img, img_to_array\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.applications import vgg19\n",
        "from keras.models import Model\n",
        "#from keras import optimizers\n",
        "from scipy.optimize import fmin_l_bfgs_b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGTxZU3LIyP_"
      },
      "source": [
        "****We all have used different camera filters that are in built in our smartphone or sometimes we ahve installed different filter apps to make our pictures look more glossy and artistic. But, have you ever wondered how these filters actually works? How this artistic looks comes?****\n",
        "\n",
        "****All this is because of an intelligent style transfer algorithm....****<br>\n",
        "****Now, What is Style Transfer??****<br>\n",
        "Style transfer is the technique of reconstructing images in the style of another image.<br> There are varoius research papers on this topic that how neural networks could be used to generate artistic style images, each author stating their own innovative way of optimization and loss function to create a artistic image generating neural network.<br>\n",
        "****After going through various works on Neural Style Transfer and how they could be constructed, In this Kernel I have explained part by part how a basic Neural style transfer is actually constructed ****"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDewocw2IyQA"
      },
      "source": [
        "![Style transfer image](https://cdn-images-1.medium.com/max/1600/1*kOQOZxBDNw4lI757soTEyQ.png)\n",
        "\n",
        "**Source - [Towards Data Science](https://towardsdatascience.com/real-time-video-neural-style-transfer-9f6f84590832)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGE5l_DGf-Y7"
      },
      "source": [
        "### Import Lib & Set Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3v4ErdIIyQA",
        "outputId": "1b978633-7094-45da-a8eb-ebfd1d1608ae"
      },
      "outputs": [],
      "source": [
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import numpy as np\n",
        "import os\n",
        "from keras import backend as K\n",
        "from keras_preprocessing.image import load_img, save_img, img_to_array\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.applications import vgg19\n",
        "from keras.models import Model\n",
        "#from keras import optimizers\n",
        "from scipy.optimize import fmin_l_bfgs_b\n",
        "#from keras.applications.vgg19 import VGG19\n",
        "#vgg19_weights = '../input/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "#vgg19 = VGG19(include_top = False, weights=vgg19_weights)\n",
        "print(os.listdir(\"/content/datasets/Styletransfer/\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_MXnlMkIyQC"
      },
      "source": [
        "## Loading the path for Base Content Image and the Style image respectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9H_CjuMIyQC"
      },
      "outputs": [],
      "source": [
        "base_image_path = ContentPath+'13.jpg'\n",
        "style_image_path = StylePath+'Pablo_Picasso/Pablo_Picasso_92.jpg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8s39lEAIyQD"
      },
      "outputs": [],
      "source": [
        "# dimensions of the generated picture.\n",
        "width, height = load_img(base_image_path).size\n",
        "img_nrows = 400\n",
        "img_ncols = int(width * img_nrows / height)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jiU7Eh7cIyQD"
      },
      "source": [
        "## This function is used to Preprocess the image with help of VGG19.\n",
        "**VGG19** is a model, with weights pre-trained on **ImageNet**.**ImageNet**, is a dataset of over 15 millions labeled high-resolution images with around 22,000 categories. ILSVRC uses a subset of ImageNet of around 1000 images in each of 1000 categories. In all, there are roughly 1.3 million training images, 50,000 validation images and 100,000 testing images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeP2kLuvIyQD"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image_path):\n",
        "    from keras.applications import vgg19\n",
        "    img = load_img(image_path, target_size=(img_nrows, img_ncols))\n",
        "    img = img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = vgg19.preprocess_input(img)\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "9O1AcTFKIyQE",
        "outputId": "637a68aa-60d8-4497-960e-3139ab828d7b"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.title(\"Base Image\",fontsize=20)\n",
        "img1 = load_img(ContentPath+'13.jpg')\n",
        "plt.imshow(img1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "uv-SfNT2IyQE",
        "outputId": "6f8dd053-37be-429f-a17d-fe5bfc5afcef"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.title(\"Style Image\",fontsize=20)\n",
        "img1 = load_img(StylePath+'Pablo_Picasso/Pablo_Picasso_92.jpg')\n",
        "plt.imshow(img1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzNqVpOrIyQE"
      },
      "outputs": [],
      "source": [
        "# get tensor representations of our images\n",
        "\n",
        "base_image = K.variable(preprocess_image(base_image_path))\n",
        "style_reference_image = K.variable(preprocess_image(style_image_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oKF1axVBIyQF",
        "outputId": "bcafbbe1-bfd5-4b3f-d853-bfc8e7223944"
      },
      "outputs": [],
      "source": [
        "K.image_data_format()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9L3wGIHjIyQF"
      },
      "source": [
        "### this will contain our generated image\n",
        "\n",
        "Think of **Variable** in tensorflow as a normal variables which we use in programming languages. We initialize variables, we can modify it later as well. Whereas **placeholder** doesnâ€™t require initial value. Placeholder simply allocates block of memory for future use. Later, we can use feed_dict to feed the data into placeholder. By default, placeholder has an unconstrained shape, which allows you to feed tensors of different shapes in a session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEWFFD4rIyQF"
      },
      "outputs": [],
      "source": [
        "# this will contain our generated image\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    combination_image = K.placeholder((1,3,img_nrows, img_ncols))\n",
        "else:\n",
        "    combination_image = K.placeholder((1,img_nrows, img_ncols,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOIWDqXLIyQG"
      },
      "outputs": [],
      "source": [
        "# combine the 3 images into a single Keras tensor\n",
        "input_tensor = K.concatenate([base_image,\n",
        "                              style_reference_image,\n",
        "                              combination_image\n",
        "                              ], axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObapZgHjIyQG"
      },
      "source": [
        "## Building the VGG19 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.applications.vgg19 import VGG19\n",
        "\n",
        "vgg19_weights = '/content/datasets/Styletransfer/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbouG33LIyQG",
        "outputId": "82abe15d-1265-4d21-c767-420861774274"
      },
      "outputs": [],
      "source": [
        "# build the VGG19 network with our 3 images as input\n",
        "# the model will be loaded with pre-trained ImageNet weights\n",
        "model = VGG19(input_tensor=input_tensor,\n",
        "              include_top = False,\n",
        "              weights=vgg19_weights)\n",
        "#model = vgg19.VGG19(input_tensor=input_tensor,\n",
        "#                    weights='imagenet', include_top=False)\n",
        "print('Model loaded.')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OSvmddXYIyQG"
      },
      "source": [
        "## Style Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JpTKgt4IyQG"
      },
      "outputs": [],
      "source": [
        "# Content layer where will pull our feature maps\n",
        "content_layers = ['block5_conv2'] \n",
        "\n",
        "# Style layer we are interested in\n",
        "style_layers = ['block1_conv1',\n",
        "                'block2_conv1',\n",
        "                'block3_conv1', \n",
        "                'block4_conv1',\n",
        "                'block5_conv1'\n",
        "               ]\n",
        "\n",
        "num_content_layers = len(content_layers)\n",
        "num_style_layers = len(style_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRGfVkg0IyQG",
        "outputId": "078d4283-d8f5-4ad4-bc30-1dc2f55a03f6"
      },
      "outputs": [],
      "source": [
        "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
        "print(outputs_dict['block5_conv2'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-4ajn0vIyQH"
      },
      "source": [
        "## The content Loss\n",
        "Given a chosen content layer **l**, the content loss is defined as the Mean Squared Error between the feature map **F** of our content image **C** and the feature map **P** of our generated image **Y**.\n",
        "![Coontent Loss](https://cdn-images-1.medium.com/max/800/1*1YfGhmzBw7EK3e8CRpZbuA.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVz6n8zNIyQH"
      },
      "outputs": [],
      "source": [
        "# an auxiliary loss function\n",
        "# designed to maintain the \"content\" of the\n",
        "# base image in the generated image\n",
        "def get_content_loss(base_content, target):\n",
        "    return K.sum(K.square(target - base_content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocCsr1qVIyQH"
      },
      "source": [
        "## The Style Loss\n",
        "To do this at first we need to, calculate the **Gram-matrix**(a matrix comprising of correlated features) for the tensors output by the style-layers. The Gram-matrix is essentially just a matrix of dot-products for the vectors of the feature activations of a style-layer.<br><br>\n",
        "If an entry in the Gram-matrix has a value close to zero then it means the two features in the given layer do not activate simultaneously for the given style-image. And vice versa, if an entry in the Gram-matrix has a large value, then it means the two features do activate simultaneously for the given style-image. We will then try and create a mixed-image that replicates this activation pattern of the style-image.\n",
        "If the feature map is a matrix **F**, then each entry in the Gram matrix **G** can be given by:\n",
        "![Gram Matrix](https://cdn-images-1.medium.com/max/800/1*5xx9KmhVb59Mxe_buOwHBA.png)\n",
        "The loss function for style is quite similar to out content loss, except that we calculate the Mean Squared Error for the Gram-matrices instead of the raw tensor-outputs from the layers.\n",
        "![Style loss](https://cdn-images-1.medium.com/max/800/1*PuYveCM2BlgFfjUCr6I_Ng.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJCe6PRXIyQH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "# the gram matrix of an image tensor (feature-wise outer product)\n",
        "def gram_matrix(input_tensor):\n",
        "    assert K.ndim(input_tensor)==3\n",
        "    #if K.image_data_format() == 'channels_first':\n",
        "    #    features = K.batch_flatten(input_tensor)\n",
        "    #else:\n",
        "    #    features = K.batch_flatten(K.permute_dimensions(input_tensor,(2,0,1)))\n",
        "    #gram = K.dot(features, K.transpose(features))\n",
        "    channels = int(input_tensor.shape[-1])\n",
        "    a = tf.reshape(input_tensor, [-1, channels])\n",
        "    n = tf.shape(a)[0]\n",
        "    gram = tf.matmul(a, a, transpose_a=True)\n",
        "    return gram#/tf.cast(n, tf.float32)\n",
        "\n",
        "def get_style_loss(style, combination):\n",
        "    assert K.ndim(style) == 3\n",
        "    assert K.ndim(combination) == 3\n",
        "    S = gram_matrix(style)\n",
        "    C = gram_matrix(combination)\n",
        "    channels = 3\n",
        "    size = img_nrows*img_ncols\n",
        "    return K.sum(K.square(S - C))#/(4.0 * (channels ** 2) * (size ** 2))\n",
        "    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jqUiZ2KxIyQH"
      },
      "source": [
        "## Feature Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMkDViBVIyQI",
        "outputId": "e41b3801-65d8-45b8-d210-4c47b625d7e6"
      },
      "outputs": [],
      "source": [
        "content_weight=0.03 \n",
        "style_weight=1.2\n",
        "# combine these loss functions into a single scalar\n",
        "loss = K.variable(0.0)\n",
        "layer_features = outputs_dict['block5_conv2']\n",
        "base_image_features = layer_features[0, :, :, :]\n",
        "combination_features = layer_features[2, :, :, :]\n",
        "print('Layer Feature for Content Layers :: '+str(layer_features))\n",
        "print('Base Image Feature :: '+str(base_image_features))\n",
        "print('Combination Image Feature for Content Layers:: '+str(combination_features)+'\\n')\n",
        "loss = loss + content_weight * get_content_loss(base_image_features,\n",
        "                                      combination_features)\n",
        "\n",
        "feature_layers = ['block1_conv1', 'block2_conv1',\n",
        "                  'block3_conv1', 'block4_conv1',\n",
        "                  'block5_conv1']\n",
        "for layer_name in feature_layers:\n",
        "    layer_features = outputs_dict[layer_name]\n",
        "    style_reference_features = layer_features[1, :, :, :]\n",
        "    combination_features = layer_features[2, :, :, :]\n",
        "    print('Layer Feature for Style Layers :: '+str(layer_features))\n",
        "    print('Style Image Feature :: '+str(style_reference_features))\n",
        "    print('Combination Image Feature for Style Layers:: '+str(combination_features)+'\\n')\n",
        "    sl = get_style_loss(style_reference_features, combination_features)\n",
        "    loss += (style_weight / len(feature_layers)) * sl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr945kbOIyQI"
      },
      "source": [
        "## Features are extracted from each layer in style layers and content layers and their overall loss is calculated from it..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT6HBloXIyQI"
      },
      "source": [
        "### This deprocess_image function is used return the original format of the Final image  after transformation which could be easily read and displayed by Matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYfXWmghIyQJ"
      },
      "outputs": [],
      "source": [
        "def deprocess_image(x):\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        x = x.reshape((3, img_nrows, img_ncols))\n",
        "        x = x.transpose((1, 2, 0))\n",
        "    else:\n",
        "        x = x.reshape((img_nrows, img_ncols, 3))\n",
        "    # Remove zero-center by mean pixel\n",
        "    x[:, :, 0] += 103.939\n",
        "    x[:, :, 1] += 116.779\n",
        "    x[:, :, 2] += 123.68\n",
        "    # 'BGR'->'RGB'\n",
        "    x = x[:, :, ::-1]\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHkxWDbfIyQJ"
      },
      "source": [
        "### Calculation of gradient with respect to loss.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggxCtBMfIyQJ",
        "outputId": "7ef9499f-da29-4d26-81d7-05bb340606f1"
      },
      "outputs": [],
      "source": [
        "# get the gradients of the generated image wrt the loss\n",
        "grads = K.gradients(loss, combination_image)\n",
        "grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBKcNt86IyQJ",
        "outputId": "be79be94-36d7-4c59-9282-a57a555443b4"
      },
      "outputs": [],
      "source": [
        "outputs = [loss]\n",
        "if isinstance(grads, (list,tuple)):\n",
        "    outputs += grads\n",
        "else:\n",
        "    outputs.append(grads)\n",
        "f_outputs = K.function([combination_image], outputs)\n",
        "f_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1IT36teIyQJ"
      },
      "source": [
        "**Athough there are various optimizers but we have used L-BFGS optimizer in this case, I have also gone through research papers where they have used ADAM optimizer to optimize the loss and get the final image.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vf7GGfb2IyQJ"
      },
      "outputs": [],
      "source": [
        "# run scipy-based optimization (L-BFGS) over the pixels of the generated image\n",
        "# so as to minimize the neural style loss\n",
        "x_opt = preprocess_image(base_image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzRUIY61IyQJ"
      },
      "outputs": [],
      "source": [
        "def eval_loss_and_grads(x):\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        x = x.reshape((1, 3, img_nrows, img_ncols))\n",
        "    else:\n",
        "        x = x.reshape((1, img_nrows, img_ncols, 3))\n",
        "    outs = f_outputs([x])\n",
        "    loss_value = outs[0]\n",
        "    if len(outs[1:]) == 1:\n",
        "        grad_values = outs[1].flatten().astype('float64')\n",
        "    else:\n",
        "        grad_values = np.array(outs[1:]).flatten().astype('float64')\n",
        "    return loss_value, grad_values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47PNMHNvIyQK"
      },
      "source": [
        "The purpose of this **Evaluator** class is to avoid the error **'numpy.ndarray' object is not callable error with optimize.minimize** while running the L-BFGS optimizer for loss minimization.<br><br>\n",
        "You should pass the function itself to minimize, instead of a evaluated value. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iVATbEzIyQK"
      },
      "outputs": [],
      "source": [
        "class Evaluator(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.loss_value = None\n",
        "        self.grads_values = None\n",
        "\n",
        "    def loss(self, x):\n",
        "        assert self.loss_value is None\n",
        "        loss_value, grad_values = eval_loss_and_grads(x)\n",
        "        self.loss_value = loss_value\n",
        "        self.grad_values = grad_values\n",
        "        return self.loss_value\n",
        "\n",
        "    def grads(self, x):\n",
        "        assert self.loss_value is not None\n",
        "        grad_values = np.copy(self.grad_values)\n",
        "        self.loss_value = None\n",
        "        self.grad_values = None\n",
        "        return grad_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpH7OKQcIyQK"
      },
      "outputs": [],
      "source": [
        "evaluator = Evaluator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUDmq5B8IyQK"
      },
      "source": [
        "In this segment we run the code upto a given iteration. Although I would not recommend you to use **maxiter** parameter in **fmin_l_bfgs_b** to set the number of iterations.  Rarther use iteration in for loop to get better results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvkBZR1TIyQK",
        "outputId": "fd9c7b75-fbf4-40c8-9796-a105bc9a3fe3"
      },
      "outputs": [],
      "source": [
        "iterations=350\n",
        "# Store our best result\n",
        "best_loss, best_img = float('inf'), None\n",
        "for i in range(iterations):\n",
        "    print('Start of iteration', i)\n",
        "    x_opt, min_val, info= fmin_l_bfgs_b(evaluator.loss, \n",
        "                                        x_opt.flatten(), \n",
        "                                        fprime=evaluator.grads,\n",
        "                                        maxfun=20,\n",
        "                                        disp=True,\n",
        "                                       )\n",
        "    print('Current loss value:', min_val)\n",
        "    if min_val < best_loss:\n",
        "        # Update best loss and best image from total loss. \n",
        "        best_loss = min_val\n",
        "        best_img = x_opt.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpTBqn4IIyQK"
      },
      "source": [
        "**The Final Image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "E9W3rt1VIyQK",
        "outputId": "d04f9c07-9089-4277-b253-1332111c79a8"
      },
      "outputs": [],
      "source": [
        "# save current generated image\n",
        "imgx = deprocess_image(best_img.copy())\n",
        "plt.imshow(imgx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbSMHFM1IyQK",
        "outputId": "343d6877-0e1c-4576-f9c9-30d2b40c446c"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(30,30))\n",
        "plt.subplot(5,5,1)\n",
        "plt.title(\"Base Image\",fontsize=20)\n",
        "img_base = load_img(base_image_path)\n",
        "plt.imshow(img_base)\n",
        "\n",
        "plt.subplot(5,5,1+1)\n",
        "plt.title(\"Style Image\",fontsize=20)\n",
        "img_style = load_img(style_image_path)\n",
        "plt.imshow(img_style)\n",
        "\n",
        "plt.subplot(5,5,1+2)\n",
        "plt.title(\"Final Image\",fontsize=20)\n",
        "plt.imshow(imgx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ijmFUkQIyQL"
      },
      "source": [
        "This function **Run_Style_Transfer** is nothing but combination of all the above code that is discussed in the above cells part by part. It returns the final image after style transfer between two images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgRTVtvuIyQL"
      },
      "outputs": [],
      "source": [
        "def preprocess_image_instantiator(image_path,img_nrows,img_ncols):\n",
        "    from keras.applications import vgg19\n",
        "    img = load_img(image_path, target_size=(img_nrows, img_ncols))\n",
        "    img = img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = vgg19.preprocess_input(img)\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8IGYYxbIyQL"
      },
      "outputs": [],
      "source": [
        "def Run_StyleTransfer(base_image_path, style_image_path):\n",
        "    \n",
        "    width, height = load_img(base_image_path).size\n",
        "    img_nrows = 400\n",
        "    img_ncols = int(width * img_nrows / height)\n",
        "    \n",
        "    base_image = K.variable(preprocess_image_instantiator(base_image_path,img_nrows,img_ncols))\n",
        "    style_reference_image = K.variable(preprocess_image_instantiator(style_image_path,img_nrows,img_ncols))\n",
        "    \n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        combination_image = K.placeholder((1,3,img_nrows, img_ncols))\n",
        "    else:\n",
        "        combination_image = K.placeholder((1,img_nrows, img_ncols,3))\n",
        "        \n",
        "    input_tensor = K.concatenate([base_image,\n",
        "                                  style_reference_image,\n",
        "                                  combination_image\n",
        "                                  ], axis=0)\n",
        "    from keras.applications.vgg19 import VGG19\n",
        "\n",
        "\n",
        "    ##------------------------- Defint vgg19 paths -------------------------##\n",
        "    vgg19_weights = '/content/datasets/Styletransfer/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "    model = VGG19(input_tensor=input_tensor,\n",
        "                  include_top = False,\n",
        "                  weights=vgg19_weights)\n",
        "    outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
        "    \n",
        "    content_weight=0.025 \n",
        "    style_weight=1.0\n",
        "    # combine these loss functions into a single scalar\n",
        "    loss = K.variable(0.0)\n",
        "    layer_features = outputs_dict['block5_conv2']\n",
        "    base_image_features = layer_features[0, :, :, :]\n",
        "    combination_features = layer_features[2, :, :, :]\n",
        "    #print('Layer Feature for Content Layers :: '+str(layer_features))\n",
        "    #print('Base Image Feature :: '+str(base_image_features))\n",
        "    #print('Combination Image Feature for Content Layers:: '+str(combination_image_features))\n",
        "    loss += content_weight * get_content_loss(base_image_features,\n",
        "                                          combination_features)\n",
        "\n",
        "    feature_layers = ['block1_conv1', 'block2_conv1',\n",
        "                      'block3_conv1', 'block4_conv1',\n",
        "                      'block5_conv1']\n",
        "    for layer_name in feature_layers:\n",
        "        layer_features = outputs_dict[layer_name]\n",
        "        style_reference_features = layer_features[1, :, :, :]\n",
        "        combination_features = layer_features[2, :, :, :]\n",
        "        #print('Layer Feature for Style Layers :: '+str(layer_features))\n",
        "        #print('Style Image Feature :: '+str(style_reference_features))\n",
        "        #print('Combination Image Feature for Style Layers:: '+str(combination_features))\n",
        "        sl = get_style_loss(style_reference_features, combination_features)\n",
        "        loss += (style_weight / len(feature_layers)) * sl\n",
        "        \n",
        "    grads = K.gradients(loss, combination_image)\n",
        "    \n",
        "    outputs = [loss]\n",
        "    if isinstance(grads, (list,tuple)):\n",
        "        outputs += grads\n",
        "    else:\n",
        "        outputs.append(grads)\n",
        "    f_outputs = K.function([combination_image], outputs)\n",
        "    \n",
        "    x_opt = preprocess_image(base_image_path)\n",
        "    \n",
        "    evaluator = Evaluator()\n",
        "    iterations=200\n",
        "    # Store our best result\n",
        "    best_loss, best_img = float('inf'), None\n",
        "    for i in range(iterations):\n",
        "        #print('Start of iteration', i)\n",
        "        x_opt, min_val, info= fmin_l_bfgs_b(evaluator.loss, \n",
        "                                            x_opt.flatten(), \n",
        "                                            fprime=evaluator.grads,\n",
        "                                            maxfun=20,\n",
        "                                            disp=True,\n",
        "                                           )\n",
        "        #print('Current loss value:', min_val)\n",
        "        if min_val < best_loss:\n",
        "            # Update best loss and best image from total loss. \n",
        "            best_loss = min_val\n",
        "            best_img = x_opt.copy()\n",
        "    imgx = deprocess_image(best_img.copy())\n",
        "    \n",
        "    return imgx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_image_path_1 = '/content/datasets/Styletransfer/image-classification/images/images/travel and  adventure/Places365_val_00005821.jpg'\n",
        "\n",
        "style_image_path_1 = '/content/datasets/Styletransfer/best-artworks-of-all-time/images/images/Paul_Klee/Paul_Klee_96.jpg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 682
        },
        "id": "YHOEZQ8UIyQL",
        "outputId": "df57df03-2dfa-4713-dd0d-25bd48a06458"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.figure(figsize=(30,30))\n",
        "plt.subplot(5,5,1)\n",
        "plt.title(\"Base Image\",fontsize=20)\n",
        "img_base = load_img(base_image_path_1)\n",
        "plt.imshow(img_base)\n",
        "\n",
        "\n",
        "plt.subplot(5,5,1+1)\n",
        "plt.title(\"Style Image\",fontsize=20)\n",
        "img_style = load_img(style_image_path_1)\n",
        "plt.imshow(img_style)\n",
        "\n",
        "plt.subplot(5,5,1+2)\n",
        "imgg = Run_StyleTransfer(base_image_path_1, style_image_path_1)\n",
        "plt.title(\"Final Image\",fontsize=20)\n",
        "plt.imshow(imgg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_image_path_2 = '/content/datasets/Styletransfer/image-classification/images/images/travel and  adventure/Places365_val_00005982.jpg'\n",
        "\n",
        "style_image_path_2 = '/content/datasets/Styletransfer/best-artworks-of-all-time/images/images/Paul_Klee/Paul_Klee_24.jpg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B305fPnXIyQL"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.figure(figsize=(30,30))\n",
        "plt.subplot(5,5,1)\n",
        "plt.title(\"Base Image\",fontsize=20)\n",
        "img_base = load_img(base_image_path_2)\n",
        "plt.imshow(img_base)\n",
        "\n",
        "\n",
        "plt.subplot(5,5,1+1)\n",
        "plt.title(\"Style Image\",fontsize=20)\n",
        "img_style = load_img(style_image_path_2)\n",
        "plt.imshow(img_style)\n",
        "\n",
        "plt.subplot(5,5,1+2)\n",
        "imga = Run_StyleTransfer(base_image_path_2, style_image_path_2)\n",
        "plt.title(\"Final Image\",fontsize=20)\n",
        "plt.imshow(imga)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_image_path_3 = '/content/datasets/Styletransfer/image-classification/images/images/travel and  adventure/Places365_val_00005752.jpg'\n",
        "\n",
        "style_image_path_3 = '/content/datasets/Styletransfer/best-artworks-of-all-time/images/images/Paul_Klee/Paul_Klee_83.jpg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HvX-iQRIyQL"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.figure(figsize=(30,30))\n",
        "plt.subplot(5,5,1)\n",
        "plt.title(\"Base Image\",fontsize=20)\n",
        "img_base = load_img(base_image_path_3)\n",
        "plt.imshow(img_base)\n",
        "\n",
        "\n",
        "plt.subplot(5,5,1+1)\n",
        "plt.title(\"Style Image\",fontsize=20)\n",
        "img_style = load_img(style_image_path_3)\n",
        "plt.imshow(img_style)\n",
        "\n",
        "plt.subplot(5,5,1+2)\n",
        "imgy = Run_StyleTransfer(base_image_path_3, style_image_path_3)\n",
        "plt.title(\"Final Image\",fontsize=20)\n",
        "plt.imshow(imgy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dsBRdX0IyQL"
      },
      "source": [
        "# Conclusion\n",
        "* In our case the Final images formed are not totally perfect, because the Style image does not totally blend with the Base content image.\n",
        "* It could be improved through icreasing the number of iteration, or by trying out a different syle transfer algorithm which could preseve the edges of the base image, or by trying out with different optimizer to minimize gradient and loss."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
